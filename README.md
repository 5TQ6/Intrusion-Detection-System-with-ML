<img width="468" height="57" alt="image" src="https://github.com/user-attachments/assets/d6c70a0f-82d6-4cf8-af90-3ad96c88a72f" />

<h1>Information about the files</h1>
<h2>1. Feature Engineering & Benchmarking Engine (main.ipynb) </h2>

This notebook serves as the preprocessing and experimental core of the project. It is designed to rigorously evaluate and compare 19 different dimensionality reduction techniques before generating the final datasets used for model training.

<h3>Key Workflows: </h3>

* **Data Cleaning & Scaling:**
    * Ingests the raw 5G-NIDD dataset.
    * Performs cleaning (infinity/NaN handling) and standard scaling (`StandardScaler`) to prepare data for sensitive algorithms like PCA and MLP.

* **Phase 1: Exhaustive Feature Selection Loop:**
    * Iteratively tests feature subsets of sizes `{1, 5, 10, 15, 20, 25, 30, 35}`.
    * For each subset, it executes a "Voting Feature Selection" protocol (utilizing Filter, Wrapper, and Embedded methods) to identify the most robust features.
    * **Output:** Automatically generates and saves subsetted datasets (CSVs) for every configuration into structured directories (e.g., `data frames/main/RF/15 features`).

* **Phase 2: Feature Transformation Tournament:**
    * Competes **PCA** (Principal Component Analysis) against **LDA** (Linear Discriminant Analysis).
    * Tests multiple variance retention thresholds: `{0.99, 0.95, 0.90}`.
    * Automatically selects the "winner" based on validation accuracy and generates the final transformed datasets.

* **Artifact Generation:**
    * Produces visualization figures for every comparison (saved in `figures/`).
    * Saves `label_classes.npy` to ensure consistent decoding of predictions across all future experiments.

<hr style="border:2px solid gray">

<h2>2. Machine Learning & Optimization Engine (ML.ipynb)</h2>

This notebook executes the Phase II (Data Engineering) and Phase III (Classification) stages of the proposed framework. It consumes the optimized, PCA-transformed datasets generated by main.ipynb to train, tune, and evaluate a comprehensive suite of 10 intrusion detection models.

<h3>Key Workflows: </h3>

* **Advanced Data Sampling Strategy:**
    * Implements the **"Hybrid-100k" Sampling Protocol**: A custom logic that combines **Random Undersampling** of majority classes (Benign, UDPFlood) to a 100,000-sample ceiling with **Oversampling** (SMOTE/Hybrid) for minority classes.
    * Benchmarking modes: Can switch dynamically between `SMOTE`, `Borderline-SMOTE`, `ADASYN`, `Undersampling`, and `Class Weights` for comparative analysis.

* **Bayesian Hyperparameter Optimization (Optuna):**
    * **Random Forest:** Optimizes `n_estimators`, `max_depth`, and split criteria to balance bias and variance.
    * **XGBoost:** Optimizes `learning_rate`, `gamma`, `reg_alpha` (L1), and `reg_lambda` (L2) to maximize the Weighted F1-Score while preventing overfitting.
    * *Note:* The use of `compute_sample_weight` is a case itself where no sampling is done; this method is used to mathematically penalize minority class misclassifications during boosting.

* **Model Zoo & Ensemble Learning:**
    * Trains a diverse set of classifiers: **Logistic Regression, Naive Bayes, KNN, Decision Tree, MLP (Neural Network), LDA, and SVM**.
    * **SVM Implementation:** Distinctively wraps `LinearSVC` in `CalibratedClassifierCV` and `OneVsRestClassifier` to enable probability estimation for multi-class ensembles.
    * **Voting Classifier:** Aggregates the predicted probabilities of all base models (Soft Voting) to improve final decision robustness.

* **Performance Visualization:**
    * Generates comprehensive performance reports (Accuracy, Precision, Recall, F1-Score).
    * Produces comparative bar charts for F1-scores across all 10 models to validate the "Hybrid-100k" hypothesis.
  
 <hr style="border:2px solid gray">

 <h2>Shared Utility Library (utils.py)</h2>
 
This script acts as the modular backbone of the framework, ensuring scientific consistency across all experiments. It centralizes the logic for data processing, complex mathematical transformations, and visualization, enforcing MDPI-style academic aesthetics for all generated figures.

<h3>Key Components: </h3>

* **Robust Data Cleaning (`clean_database`):**
    * Implements **"Anti-Shortcut Learning"**: Automatically strips network-specific topology features (e.g., `sVid`, `dVid`, `Seq`) that leads to overfitting in static tests.
    * **Skewness/Scaling**: Applies `PowerTransformer` (Yeo-Johnson) to normalize non-Gaussian traffic distributions if `fix_skewness` is set to True, or applies a Standard scaler if `do_scale` is set to True.
    * **Data Leakage Prevention**: Strictly calculates imputation statistics (medians) and encoders solely on the **Training Set** before applying them to Validation/Test sets.

* **Advanced Feature Selection Algorithms:**
    * Contains the core logic for **19 Feature Selection Methods**, including computationally intensive Wrappers:
        * **Evolutionary Algorithms**: Genetic Algorithms (GA) and Simulated Annealing.
        * **Stepwise Selection**: Forward, Backward, and Floating Stepwise.
    * **Voting Mechanism**: Logic to compare validation accuracy across methods and automatically select the winner.

* **Custom Hybrid Sampling Engine:**
    * The source code for the **`hybrid_100k`** strategy resides here. It dynamically calculates class distributions and builds a `Pipeline` combining `RandomUnderSampler` (for majority classes > 100k) and `SMOTE` (for minority classes < 100k).

* **Standardized Evaluation:**
    * `print_evaluation_metrics`: Automatically logs Accuracy, Weighted Precision/Recall/F1, and Latency to text files.
    * **MDPI-Style Plots**: `apply_ieee_style()` sets global matplotlib parameters (font sizes, dimension 8.5cm width) to ensure all Confusion Matrices and Bar Charts meet journal publication standards.
