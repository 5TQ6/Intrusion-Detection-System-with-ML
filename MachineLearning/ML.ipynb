{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd4c716",
   "metadata": {},
   "source": [
    "<h1 style=\"color:yellow;\">Concurrent Data - Machine Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6f1fe",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84d756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "from utils import print_evaluation_metrics, log_metrics, plot_individual_metrics, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d40c1da",
   "metadata": {},
   "source": [
    "# Load the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad45f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Define the directory where your data is saved\n",
    "save_dir = os.path.join('..', 'data frames', 'main', 'MLP', 'PCA_0.99')\n",
    "\n",
    "X_train = pd.read_csv(os.path.join(save_dir, 'X_train_PCA_0.99.csv'))\n",
    "X_val = pd.read_csv(os.path.join(save_dir, 'X_val_PCA_0.99.csv'))\n",
    "X_test = pd.read_csv(os.path.join(save_dir, 'X_test_PCA_0.99.csv'))\n",
    "\n",
    "# 3. Load the target vectors (y)\n",
    "# Use .values.ravel() to convert the DataFrame to a 1D array (expected for labels)\n",
    "y_train = pd.read_csv(os.path.join(save_dir, 'y_train_PCA_0.99.csv')).values.ravel()\n",
    "y_val = pd.read_csv(os.path.join(save_dir, 'y_val_PCA_0.99.csv')).values.ravel()\n",
    "y_test = pd.read_csv(os.path.join(save_dir, 'y_test_PCA_0.99.csv')).values.ravel()\n",
    "\n",
    "# 4. Load the label encoder classes\n",
    "label_classes = np.load(os.path.join(save_dir, 'label_classes_PCA_0.99.npy'), allow_pickle=True)\n",
    "\n",
    "print(\"All datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Reconstruct the encoder from the saved classes\n",
    "output_encoder = LabelEncoder()\n",
    "output_encoder.classes_ = label_classes\n",
    "output_encoder.fit(label_classes) # Sometimes needed to initialize internal dicts\n",
    "\n",
    "print(f\"Encoder restored. Classes: {output_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7adf92b",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c563b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options available are : none, smote, adasyn, borderline_smote, hybrid_100k, undersample\n",
    "# Here you can specify the sampling method used, note that you should first remove \"class_weight = 'balanced'\" from the model training if you are using a sampling method.\n",
    "sampling_method = 'hybrid_100k' \n",
    "version = sampling_method + '_Optuna' + 'PCA99'\n",
    "plot_distributions = True\n",
    "model_results= []\n",
    "\n",
    "results_dir = os.path.join('figures', version, 'results')\n",
    "evaluation_dir = os.path.join('figures', version, 'evaluatoin charts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72afd0b",
   "metadata": {},
   "source": [
    "# Process the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24651db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_proc, X_val_proc, X_test_proc, y_train_proc, y_val_proc, y_test_proc = preprocessing(\n",
    "        X_train, X_val, X_test,\n",
    "        y_train, y_val, y_test,\n",
    "        output_encoder,\n",
    "        evaluation_dir,       # Pass save_dir here so plots (like class dist) save to the specific folder\n",
    "        version, \n",
    "        sampling_method, \n",
    "        plot_distributions\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294492bf",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1dab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import optuna\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725003c6",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3538c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LogisticRegression(solver='saga', max_iter=1000, random_state=42, class_weight='balanced')\n",
    "\n",
    "start_time = time.time()\n",
    "linear_model.fit(X_train_proc, y_train_proc)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred_lr = linear_model.predict(X_val_proc)\n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "# arguments for evaluation function\n",
    "results_file_name = 'logistic_regression_results.txt'\n",
    "cm_title = 'Logistic Regression Confusion Matrix'\n",
    "\n",
    "print(\"Logistic Regression Evaluation:\")\n",
    "accuracy, precision, recall, f1 = print_evaluation_metrics(y_val_proc, y_pred_lr, training_time, prediction_time, output_encoder, results_dir, version, results_file_name, cm_title)\n",
    "log_metrics(model_results, 'LR', accuracy, precision, recall, f1, training_time, prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0284d5",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f94209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rf(trial):\n",
    "    # 1. Define the search space\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 50)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 15)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    \n",
    "    # 2. Initialize model with trial parameters\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    # 3. Train\n",
    "    clf.fit(X_train_proc, y_train_proc)\n",
    "    \n",
    "    # 4. Evaluate (Maximize F1 Weighted)\n",
    "    y_pred = clf.predict(X_val_proc)\n",
    "    score = f1_score(y_val_proc, y_pred, average='weighted')\n",
    "    return score\n",
    "\n",
    "# Create Study and Optimize\n",
    "print(\"Optimizing Random Forest with Optuna...\")\n",
    "study_rf = optuna.create_study(direction='maximize')\n",
    "study_rf.optimize(objective_rf, n_trials=40) # Increase n_trials (e.g., 50 or 100) for better results\n",
    "\n",
    "print(\"Best Random Forest Params:\", study_rf.best_params)\n",
    "\n",
    "# Train Final Model with Best Params\n",
    "best_params_rf = study_rf.best_params\n",
    "rnd_forest = RandomForestClassifier(\n",
    "    **best_params_rf,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "rnd_forest.fit(X_train_proc, y_train_proc)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred_rf = rnd_forest.predict(X_val_proc)\n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "# Evaluation\n",
    "results_file_name = 'random_forest_optuna_results.txt'\n",
    "cm_title = 'Random Forest (Optuna) Confusion Matrix'\n",
    "\n",
    "print(\"Random Forest (Optuna) Evaluation:\")\n",
    "accuracy, precision, recall, f1 = print_evaluation_metrics(y_val_proc, y_pred_rf, training_time, prediction_time, output_encoder, results_dir, version, results_file_name, cm_title)\n",
    "log_metrics(model_results, 'RF', accuracy, precision, recall, f1, training_time, prediction_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244acc30",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88029554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute sample weights once to save time inside the loop\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train_proc)\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    # 1. Define search space\n",
    "    params = {\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': len(output_encoder.classes_),\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "    }\n",
    "    \n",
    "    # 2. Initialize model\n",
    "    clf = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    # 3. Train (using sample weights)\n",
    "    clf.fit(X_train_proc, y_train_proc, sample_weight=sample_weights)\n",
    "    \n",
    "    # 4. Evaluate\n",
    "    y_pred = clf.predict(X_val_proc)\n",
    "    score = f1_score(y_val_proc, y_pred, average='weighted')\n",
    "    return score\n",
    "\n",
    "# Create Study and Optimize\n",
    "print(\"Optimizing XGBoost with Optuna...\")\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=40) # Increase n_trials for better results\n",
    "\n",
    "print(\"Best XGBoost Params:\", study_xgb.best_params)\n",
    "\n",
    "# Train Final Model with Best Params\n",
    "best_params_xgb = study_xgb.best_params\n",
    "# Re-add fixed parameters that aren't in best_params\n",
    "best_params_xgb.update({\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': len(output_encoder.classes_),\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42\n",
    "})\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(**best_params_xgb)\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_model.fit(X_train_proc, y_train_proc, sample_weight=sample_weights)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred_xgb = xgb_model.predict(X_val_proc)\n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "# Evaluation\n",
    "results_file_name = 'xgboost_optuna_results.txt'\n",
    "cm_title = 'XGBoost (Optuna) Confusion Matrix'\n",
    "\n",
    "print(\"XGBoost (Optuna) Evaluation:\")\n",
    "accuracy, precision, recall, f1 = print_evaluation_metrics(y_val_proc, y_pred_xgb, training_time, prediction_time, output_encoder, results_dir, version, results_file_name, cm_title)\n",
    "log_metrics(model_results, 'XGB', accuracy, precision, recall, f1, training_time, prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naive-bayes-markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naive-bayes-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Naive Bayes classifier\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "nb_model.fit(X_train_proc, y_train_proc)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Make predictions\n",
    "start_time = time.time()\n",
    "y_pred_nb = nb_model.predict(X_val_proc)\n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "# arguments for evaluation function\n",
    "results_file_name = 'naive_bayes_results.txt'\n",
    "cm_title = 'Naive Bayes Confusion Matrix'\n",
    "\n",
    "print(\"Naive Bayes Evaluation:\")\n",
    "accuracy, precision, recall, f1 = print_evaluation_metrics(y_val_proc, y_pred_nb, training_time, prediction_time, output_encoder, results_dir, version, results_file_name, cm_title)\n",
    "log_metrics(model_results, 'NB', accuracy, precision, recall, f1, training_time, prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59fa344",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c4fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Define Linear SVM\n",
    "# dual=False is generally preferred when n_samples > n_features.\n",
    "linear_svc = LinearSVC(dual=False, random_state=42, max_iter=200000, class_weight='balanced')\n",
    "\n",
    "# 2. Wrap in CalibratedClassifierCV to enable predict_proba\n",
    "# This is required for the model to be compatible with a soft-voting ensemble.\n",
    "calibrated_svc = CalibratedClassifierCV(estimator=linear_svc, method='sigmoid', cv=3)\n",
    "\n",
    "# 3. Wrap in OneVsRestClassifier\n",
    "# n_jobs=4 uses 4 CPU cores to train the per-class models in parallel.\n",
    "ovr_classifier = OneVsRestClassifier(estimator=calibrated_svc, n_jobs=4)\n",
    "\n",
    "# Train\n",
    "print(\"Training One-vs-Rest Linear SVM (Calibrated)...\")\n",
    "start_time = time.time()\n",
    "ovr_classifier.fit(X_train_proc, y_train_proc)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Predict\n",
    "print(\"Predicting...\")\n",
    "start_time = time.time()\n",
    "y_pred_svm = ovr_classifier.predict(X_val_proc)\n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "# Arguments for evaluation\n",
    "results_file_name = 'svm_linear_ovr_results.txt'\n",
    "cm_title = 'SVM (Linear OvR) Confusion Matrix'\n",
    "\n",
    "print(\"SVM (Linear OvR) Evaluation:\")\n",
    "# Ensure print_evaluation_metrics and log_metrics are available from utils.py\n",
    "accuracy, precision, recall, f1 = print_evaluation_metrics(\n",
    "    y_val_proc, y_pred_svm, training_time, prediction_time, \n",
    "    output_encoder, results_dir, version, results_file_name, cm_title\n",
    ")\n",
    "\n",
    "# Log metrics\n",
    "log_metrics(model_results, 'SVM', accuracy, precision, recall, f1, training_time, prediction_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5732877",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e25ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create k-NN classifier\n",
    "# n_neighbors=5 is standard. n_jobs=-1 is CRITICAL for speed.\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "\n",
    "# Train\n",
    "print(\"Training k-NN ...\")\n",
    "start_time = time.time()\n",
    "knn_model.fit(X_train_proc, y_train_proc)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Predict\n",
    "# WARNING: k-NN is slow at prediction time!\n",
    "print(\"Predicting k-NN ...\")\n",
    "start_time = time.time()\n",
    "y_pred_knn = knn_model.predict(X_val_proc)\n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "# Evaluation\n",
    "results_file_name = 'knn_results.txt'\n",
    "cm_title = 'k-NN Confusion Matrix'\n",
    "\n",
    "print(\"k-NN Evaluation:\")\n",
    "accuracy, precision, recall, f1 = print_evaluation_metrics(y_val_proc, y_pred_knn, training_time, prediction_time, output_encoder, results_dir, version, results_file_name, cm_title)\n",
    "log_metrics(model_results, 'KNN', accuracy, precision, recall, f1, training_time, prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f97fe5f",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5712fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree\n",
    "# max_depth=None means it grows fully (can overfit). \n",
    "dt_model = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Train\n",
    "print(\"Training Decision Tree...\")\n",
    "start_time = time.time()\n",
    "dt_model.fit(X_train_proc, y_train_proc)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Predict\n",
    "print(\"Predicting...\")\n",
    "start_time = time.time()\n",
    "y_pred_dt = dt_model.predict(X_val_proc)\n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "# Evaluation\n",
    "results_file_name = 'decision_tree_results.txt'\n",
    "cm_title = 'Decision Tree Confusion Matrix'\n",
    "\n",
    "print(\"Decision Tree Evaluation:\")\n",
    "accuracy, precision, recall, f1 = print_evaluation_metrics(y_val_proc, y_pred_dt, training_time, prediction_time, output_encoder, results_dir, version, results_file_name, cm_title)\n",
    "log_metrics(model_results, 'DT', accuracy, precision, recall, f1, training_time, prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28deb824",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f5af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP (Shallow Neural Network)\n",
    "# hidden_layer_sizes=(100, 50) means 2 layers. max_iter=300 ensures convergence.\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(128, 96, 64, 48), max_iter=300, random_state=42, activation='relu', solver='adam', alpha=0.005, verbose=True)\n",
    "\n",
    "print(\"Training MLP Classifier...\")\n",
    "start_time = time.time()\n",
    "mlp_model.fit(X_train_proc, y_train_proc)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"Predicting...\")\n",
    "start_time = time.time()\n",
    "y_pred_mlp = mlp_model.predict(X_val_proc)\n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "# Evaluation\n",
    "results_file_name = 'mlp_results.txt'\n",
    "cm_title = 'MLP Classifier Confusion Matrix'\n",
    "\n",
    "print(\"MLP Evaluation:\")\n",
    "accuracy, precision, recall, f1 = print_evaluation_metrics(y_val_proc, y_pred_mlp, training_time, prediction_time, output_encoder, results_dir, version, results_file_name, cm_title)\n",
    "log_metrics(model_results, 'MLP', accuracy, precision, recall, f1, training_time, prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7cf09",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08976b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LDA\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "\n",
    "print(\"Training LDA...\")\n",
    "start_time = time.time()\n",
    "lda_model.fit(X_train_proc, y_train_proc)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"Predicting...\")\n",
    "start_time = time.time()\n",
    "y_pred_lda = lda_model.predict(X_val_proc)\n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "# Evaluation\n",
    "results_file_name = 'lda_results.txt'\n",
    "cm_title = 'LDA Confusion Matrix'\n",
    "\n",
    "print(\"LDA Evaluation:\")\n",
    "accuracy, precision, recall, f1 = print_evaluation_metrics(y_val_proc, y_pred_lda, training_time, prediction_time, output_encoder, results_dir, version, results_file_name, cm_title)\n",
    "log_metrics(model_results, 'LDA', accuracy, precision, recall, f1, training_time, prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a131a77",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18cc3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the models to combine (Estimators)\n",
    "# We use the models you've already defined above (make sure to run their cells first!)\n",
    "estimators = [\n",
    "    ('rf', rnd_forest),\n",
    "    ('xgb', xgb_model),\n",
    "    ('svm', ovr_classifier),\n",
    "    ('knn', knn_model),\n",
    "    ('dt', dt_model),\n",
    "    ('mlp', mlp_model)\n",
    "]\n",
    "\n",
    "# 2. Create Voting Classifier\n",
    "# voting='soft' averages the probabilities (usually better). 'hard' counts votes.\n",
    "voting_model = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)\n",
    "\n",
    "print(\"Training Voting Classifier (Ensemble)...\")\n",
    "start_time = time.time()\n",
    "voting_model.fit(X_train_proc, y_train_proc)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"Predicting...\")\n",
    "start_time = time.time()\n",
    "y_pred_voting = voting_model.predict(X_val_proc)\n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "# Evaluation\n",
    "results_file_name = 'voting_ensemble_results.txt'\n",
    "cm_title = 'Voting Ensemble Confusion Matrix'\n",
    "\n",
    "print(\"Voting Ensemble Evaluation:\")\n",
    "accuracy, precision, recall, f1 = print_evaluation_metrics(y_val_proc, y_pred_voting, training_time, prediction_time, output_encoder, results_dir, version, results_file_name, cm_title)\n",
    "log_metrics(model_results, 'VC', accuracy, precision, recall, f1, training_time, prediction_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8526e953",
   "metadata": {},
   "source": [
    "# Bar charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c16e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_individual_metrics(model_results, evaluation_dir, version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
